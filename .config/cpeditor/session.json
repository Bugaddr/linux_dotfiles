{
    "currentIndex": 1,
    "tabs": [
        {
            "checkerIndex": 0,
            "customCheckers": [
            ],
            "customCompileCommand": "",
            "customTimeLimit": -1,
            "editorAnchor": 0,
            "editorCursor": 0,
            "editorText": "# -*- coding: utf-8 -*-\n\nimport json\nimport hashlib\nimport os\nimport platform\n \nlicense = {\n    \"header\": {\"version\": 1},\n    \"payload\": {\n        \"name\": \"MAGA\",\n        \"email\": \"MAGA@MAGA.com\",\n        \"licenses\": [\n            {\n                \"id\": \"48-2137-ACAB-99\",\n                \"edition_id\": \"ida-pro\",\n                \"description\": \"license\",\n                \"license_type\": \"named\",\n                \"product\": \"IDA\",\n                \"product_id\": \"IDAPRO\",\n                \"seats\": 1,\n                \"start_date\": \"2024-08-10 00:00:00\",\n                \"end_date\": \"2033-12-31 23:59:59\",\n                \"issued_on\": \"2024-08-10 00:00:00\",\n                \"owner\": \"MAGA\",\n                \"add_ons\": [],\n                \"features\": [],\n            }\n        ],\n    },\n}\n \ndef add_every_addon(license):\n    platforms = [\n        \"W\",  # Windows\n        \"L\",  # Linux\n        \"M\",  # macOS\n    ]\n    addons = [\n        \"HEXX86\",\n        \"HEXX64\",\n        \"HEXARM\",\n        \"HEXARM64\",\n        \"HEXMIPS\",\n        \"HEXMIPS64\",\n        \"HEXPPC\",\n        \"HEXPPC64\",\n        \"HEXRV64\",\n        \"HEXARC\",\n        \"HEXARC64\",\n    ]\n \n    i = 0\n    for addon in addons:\n        i += 1\n        license[\"payload\"][\"licenses\"][0][\"add_ons\"].append(\n            {\n                \"id\": f\"48-1337-0000-{i:02}\",\n                \"code\": addon,\n                \"owner\": license[\"payload\"][\"licenses\"][0][\"id\"],\n                \"start_date\": \"2024-08-10 00:00:00\",\n                \"end_date\": \"2033-12-31 23:59:59\",\n            }\n        )\n    \nadd_every_addon(license)\n \ndef json_stringify_alphabetical(obj):\n    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))\n \ndef buf_to_bigint(buf):\n    return int.from_bytes(buf, byteorder=\"little\")\n \ndef bigint_to_buf(i):\n    return i.to_bytes((i.bit_length() + 7) // 8, byteorder=\"little\")\n \n# Yup, you only have to patch 5c -> cb in libida64.so\npub_modulus_hexrays = buf_to_bigint(\n    bytes.fromhex(\n        \"edfd425cf978546e8911225884436c57140525650bcf6ebfe80edbc5fb1de68f4c66c29cb22eb668788afcb0abbb718044584b810f8970cddf227385f75d5dddd91d4f18937a08aa83b28c49d12dc92e7505bb38809e91bd0fbd2f2e6ab1d2e33c0c55d5bddd478ee8bf845fcef3c82b9d2929ecb71f4d1b3db96e3a8e7aaf93\"\n    )\n)\npub_modulus_patched = buf_to_bigint(\n    bytes.fromhex(\n        \"edfd42cbf978546e8911225884436c57140525650bcf6ebfe80edbc5fb1de68f4c66c29cb22eb668788afcb0abbb718044584b810f8970cddf227385f75d5dddd91d4f18937a08aa83b28c49d12dc92e7505bb38809e91bd0fbd2f2e6ab1d2e33c0c55d5bddd478ee8bf845fcef3c82b9d2929ecb71f4d1b3db96e3a8e7aaf93\"\n    )\n)\n \nprivate_key = buf_to_bigint(\n    bytes.fromhex(\n        \"77c86abbb7f3bb134436797b68ff47beb1a5457816608dbfb72641814dd464dd640d711d5732d3017a1c4e63d835822f00a4eab619a2c4791cf33f9f57f9c2ae4d9eed9981e79ac9b8f8a411f68f25b9f0c05d04d11e22a3a0d8d4672b56a61f1532282ff4e4e74759e832b70e98b9d102d07e9fb9ba8d15810b144970029874\"\n    )\n)\n \ndef decrypt(message):\n    decrypted = pow(buf_to_bigint(message), exponent, pub_modulus_patched)\n    decrypted = bigint_to_buf(decrypted)\n    return decrypted[::-1]\n \ndef encrypt(message):\n    encrypted = pow(buf_to_bigint(message[::-1]), private_key, pub_modulus_patched)\n    encrypted = bigint_to_buf(encrypted)\n    return encrypted\n \nexponent = 0x13\n \ndef sign_hexlic(payload: dict) -> str:\n    data = {\"payload\": payload}\n    data_str = json_stringify_alphabetical(data)\n \n    buffer = bytearray(128)\n    # first 33 bytes are random\n    for i in range(33):\n        buffer[i] = 0x42\n \n    # compute sha256 of the data\n    sha256 = hashlib.sha256()\n    sha256.update(data_str.encode())\n    digest = sha256.digest()\n \n    # copy the sha256 digest to the buffer\n    for i in range(32):\n        buffer[33 + i] = digest[i]\n \n    # encrypt the buffer\n    encrypted = encrypt(buffer)\n \n    return encrypted.hex().upper()\n \ndef patch(filename):\n    if not os.path.exists(filename):\n        print(f\"Skip: {filename} - didn't find\")\n        return\n \n    with open(filename, \"rb\") as f:\n        data = f.read()\n \n        if data.find(bytes.fromhex(\"EDFD42CBF978\")) != -1:\n            print(f\"Patch: {filename} - looks to be already patched :)\")\n            return\n \n        if data.find(bytes.fromhex(\"EDFD425CF978\")) == -1:\n            print(f\"Patch: {filename} - doesn't contain the original modulus.\")\n            return\n \n        data = data.replace(\n            bytes.fromhex(\"EDFD425CF978\"), bytes.fromhex(\"EDFD42CBF978\")\n        )\n     \n    with open(filename, \"wb\") as f:\n        f.write(data)\n\n    print(f\"Patch: {filename} - OK\")\n \nlicense[\"signature\"] = sign_hexlic(license[\"payload\"])\nserialized = json_stringify_alphabetical(license)\n \nfilename = \"idapro.hexlic\"\nwith open(filename, \"w\") as f:\n    f.write(serialized)\n \nprint(f\"\\nSaved new license to {filename}!\\n\")\n \nos_name = platform.system().lower()\nif os_name == 'windows':\n    patch(\"ida.dll\")\n    patch(\"ida32.dll\")\nelif os_name == 'linux':\n    patch(\"libida.so\")\n    patch(\"libida32.so\")\nelif os_name == 'darwin':\n    patch(\"libida.dylib\")\n    patch(\"libida32.dylib\")",
            "expected": [
                ""
            ],
            "filePath": "/home/bugaddr/Downloads/idakeygen_9.py",
            "horizontalScrollBarValue": 0,
            "input": [
                ""
            ],
            "isLanguageSet": true,
            "language": "Python",
            "problemURL": "",
            "savedText": "# -*- coding: utf-8 -*-\n\nimport json\nimport hashlib\nimport os\nimport platform\n \nlicense = {\n    \"header\": {\"version\": 1},\n    \"payload\": {\n        \"name\": \"MAGA\",\n        \"email\": \"MAGA@MAGA.com\",\n        \"licenses\": [\n            {\n                \"id\": \"48-2137-ACAB-99\",\n                \"edition_id\": \"ida-pro\",\n                \"description\": \"license\",\n                \"license_type\": \"named\",\n                \"product\": \"IDA\",\n                \"product_id\": \"IDAPRO\",\n                \"seats\": 1,\n                \"start_date\": \"2024-08-10 00:00:00\",\n                \"end_date\": \"2033-12-31 23:59:59\",\n                \"issued_on\": \"2024-08-10 00:00:00\",\n                \"owner\": \"MAGA\",\n                \"add_ons\": [],\n                \"features\": [],\n            }\n        ],\n    },\n}\n \ndef add_every_addon(license):\n    platforms = [\n        \"W\",  # Windows\n        \"L\",  # Linux\n        \"M\",  # macOS\n    ]\n    addons = [\n        \"HEXX86\",\n        \"HEXX64\",\n        \"HEXARM\",\n        \"HEXARM64\",\n        \"HEXMIPS\",\n        \"HEXMIPS64\",\n        \"HEXPPC\",\n        \"HEXPPC64\",\n        \"HEXRV64\",\n        \"HEXARC\",\n        \"HEXARC64\",\n    ]\n \n    i = 0\n    for addon in addons:\n        i += 1\n        license[\"payload\"][\"licenses\"][0][\"add_ons\"].append(\n            {\n                \"id\": f\"48-1337-0000-{i:02}\",\n                \"code\": addon,\n                \"owner\": license[\"payload\"][\"licenses\"][0][\"id\"],\n                \"start_date\": \"2024-08-10 00:00:00\",\n                \"end_date\": \"2033-12-31 23:59:59\",\n            }\n        )\n    \nadd_every_addon(license)\n \ndef json_stringify_alphabetical(obj):\n    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))\n \ndef buf_to_bigint(buf):\n    return int.from_bytes(buf, byteorder=\"little\")\n \ndef bigint_to_buf(i):\n    return i.to_bytes((i.bit_length() + 7) // 8, byteorder=\"little\")\n \n# Yup, you only have to patch 5c -> cb in libida64.so\npub_modulus_hexrays = buf_to_bigint(\n    bytes.fromhex(\n        \"edfd425cf978546e8911225884436c57140525650bcf6ebfe80edbc5fb1de68f4c66c29cb22eb668788afcb0abbb718044584b810f8970cddf227385f75d5dddd91d4f18937a08aa83b28c49d12dc92e7505bb38809e91bd0fbd2f2e6ab1d2e33c0c55d5bddd478ee8bf845fcef3c82b9d2929ecb71f4d1b3db96e3a8e7aaf93\"\n    )\n)\npub_modulus_patched = buf_to_bigint(\n    bytes.fromhex(\n        \"edfd42cbf978546e8911225884436c57140525650bcf6ebfe80edbc5fb1de68f4c66c29cb22eb668788afcb0abbb718044584b810f8970cddf227385f75d5dddd91d4f18937a08aa83b28c49d12dc92e7505bb38809e91bd0fbd2f2e6ab1d2e33c0c55d5bddd478ee8bf845fcef3c82b9d2929ecb71f4d1b3db96e3a8e7aaf93\"\n    )\n)\n \nprivate_key = buf_to_bigint(\n    bytes.fromhex(\n        \"77c86abbb7f3bb134436797b68ff47beb1a5457816608dbfb72641814dd464dd640d711d5732d3017a1c4e63d835822f00a4eab619a2c4791cf33f9f57f9c2ae4d9eed9981e79ac9b8f8a411f68f25b9f0c05d04d11e22a3a0d8d4672b56a61f1532282ff4e4e74759e832b70e98b9d102d07e9fb9ba8d15810b144970029874\"\n    )\n)\n \ndef decrypt(message):\n    decrypted = pow(buf_to_bigint(message), exponent, pub_modulus_patched)\n    decrypted = bigint_to_buf(decrypted)\n    return decrypted[::-1]\n \ndef encrypt(message):\n    encrypted = pow(buf_to_bigint(message[::-1]), private_key, pub_modulus_patched)\n    encrypted = bigint_to_buf(encrypted)\n    return encrypted\n \nexponent = 0x13\n \ndef sign_hexlic(payload: dict) -> str:\n    data = {\"payload\": payload}\n    data_str = json_stringify_alphabetical(data)\n \n    buffer = bytearray(128)\n    # first 33 bytes are random\n    for i in range(33):\n        buffer[i] = 0x42\n \n    # compute sha256 of the data\n    sha256 = hashlib.sha256()\n    sha256.update(data_str.encode())\n    digest = sha256.digest()\n \n    # copy the sha256 digest to the buffer\n    for i in range(32):\n        buffer[33 + i] = digest[i]\n \n    # encrypt the buffer\n    encrypted = encrypt(buffer)\n \n    return encrypted.hex().upper()\n \ndef patch(filename):\n    if not os.path.exists(filename):\n        print(f\"Skip: {filename} - didn't find\")\n        return\n \n    with open(filename, \"rb\") as f:\n        data = f.read()\n \n        if data.find(bytes.fromhex(\"EDFD42CBF978\")) != -1:\n            print(f\"Patch: {filename} - looks to be already patched :)\")\n            return\n \n        if data.find(bytes.fromhex(\"EDFD425CF978\")) == -1:\n            print(f\"Patch: {filename} - doesn't contain the original modulus.\")\n            return\n \n        data = data.replace(\n            bytes.fromhex(\"EDFD425CF978\"), bytes.fromhex(\"EDFD42CBF978\")\n        )\n     \n    with open(filename, \"wb\") as f:\n        f.write(data)\n\n    print(f\"Patch: {filename} - OK\")\n \nlicense[\"signature\"] = sign_hexlic(license[\"payload\"])\nserialized = json_stringify_alphabetical(license)\n \nfilename = \"idapro.hexlic\"\nwith open(filename, \"w\") as f:\n    f.write(serialized)\n \nprint(f\"\\nSaved new license to {filename}!\\n\")\n \nos_name = platform.system().lower()\nif os_name == 'windows':\n    patch(\"ida.dll\")\n    patch(\"ida32.dll\")\nelif os_name == 'linux':\n    patch(\"libida.so\")\n    patch(\"libida32.so\")\nelif os_name == 'darwin':\n    patch(\"libida.dylib\")\n    patch(\"libida32.dylib\")",
            "testCaseSplitterStates": [
                [
                    197,
                    182,
                    199
                ]
            ],
            "testcasesIsShow": [
                true
            ],
            "timestamp": 1756755660237,
            "untitledIndex": 1,
            "verticalScrollbarValue": 0
        },
        {
            "checkerIndex": 0,
            "customCheckers": [
            ],
            "customCompileCommand": "",
            "customTimeLimit": -1,
            "editorAnchor": 0,
            "editorCursor": 0,
            "editorText": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# --- 1. Setup Data Paths and Parameters ---\n# Replace this with the path to your extracted dataset folder\ndataset_path = 'mri_images/train'\n\nIMG_WIDTH, IMG_HEIGHT = 128, 128\nBATCH_SIZE = 32\n\n# --- 2. Create Image Data Generators ---\n# This will automatically load images from the folders, resize them, \n# and prepare them for the model. It also splits the data into training (80%)\n# and validation (20%) sets.\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,  # Normalize pixel values to be between 0 and 1\n    validation_split=0.2  # Use 20% of the data for validation\n)\n\ntrain_generator = datagen.flow_from_directory(\n    dataset_path,\n    target_size=(IMG_WIDTH, IMG_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical', # For multi-class classification\n    subset='training' # Specify this is the training set\n)\n\nvalidation_generator = datagen.flow_from_directory(\n    dataset_path,\n    target_size=(IMG_WIDTH, IMG_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='validation' # Specify this is the validation set\n)\n\n# --- 3. Build the CNN Model ---\n# This is a simple CNN architecture.\nmodel = Sequential([\n    # 1st Convolutional Block\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n    MaxPooling2D((2, 2)),\n\n    # 2nd Convolutional Block\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    \n    # 3rd Convolutional Block\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    # Flatten the results to feed into a dense layer\n    Flatten(),\n\n    # Dense Layer\n    Dense(128, activation='relu'),\n    \n    # Output Layer\n    # The number of units (4) must match the number of classes.\n    # 'softmax' is used for multi-class classification.\n    Dense(4, activation='softmax') \n])\n\n# --- 4. Compile the Model ---\n# This configures the model for training.\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Print a summary of the model architecture\nmodel.summary()\n\n# --- 5. Train the Model ---\n# This is where the model learns from the data.\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    epochs=15 # You can increase the number of epochs for better performance\n)\n\n# --- 6. Save the Model ---\n# Save the trained model to a file for later use.\nmodel.save('alzheimers_cnn_model.h5')\n\nprint(\"Model training complete and saved as alzheimers_cnn_model.h5\")\n",
            "expected": [
                ""
            ],
            "filePath": "/home/bugaddr/Junk/cnn/main.py",
            "horizontalScrollBarValue": 0,
            "input": [
                ""
            ],
            "isLanguageSet": true,
            "language": "Python",
            "problemURL": "",
            "savedText": "import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# --- 1. Setup Data Paths and Parameters ---\n# Replace this with the path to your extracted dataset folder\ndataset_path = 'mri_images/train'\n\nIMG_WIDTH, IMG_HEIGHT = 128, 128\nBATCH_SIZE = 32\n\n# --- 2. Create Image Data Generators ---\n# This will automatically load images from the folders, resize them, \n# and prepare them for the model. It also splits the data into training (80%)\n# and validation (20%) sets.\n\ndatagen = ImageDataGenerator(\n    rescale=1./255,  # Normalize pixel values to be between 0 and 1\n    validation_split=0.2  # Use 20% of the data for validation\n)\n\ntrain_generator = datagen.flow_from_directory(\n    dataset_path,\n    target_size=(IMG_WIDTH, IMG_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical', # For multi-class classification\n    subset='training' # Specify this is the training set\n)\n\nvalidation_generator = datagen.flow_from_directory(\n    dataset_path,\n    target_size=(IMG_WIDTH, IMG_HEIGHT),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    subset='validation' # Specify this is the validation set\n)\n\n# --- 3. Build the CNN Model ---\n# This is a simple CNN architecture.\nmodel = Sequential([\n    # 1st Convolutional Block\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)),\n    MaxPooling2D((2, 2)),\n\n    # 2nd Convolutional Block\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    \n    # 3rd Convolutional Block\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n\n    # Flatten the results to feed into a dense layer\n    Flatten(),\n\n    # Dense Layer\n    Dense(128, activation='relu'),\n    \n    # Output Layer\n    # The number of units (4) must match the number of classes.\n    # 'softmax' is used for multi-class classification.\n    Dense(4, activation='softmax') \n])\n\n# --- 4. Compile the Model ---\n# This configures the model for training.\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Print a summary of the model architecture\nmodel.summary()\n\n# --- 5. Train the Model ---\n# This is where the model learns from the data.\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // BATCH_SIZE,\n    epochs=15 # You can increase the number of epochs for better performance\n)\n\n# --- 6. Save the Model ---\n# Save the trained model to a file for later use.\nmodel.save('alzheimers_cnn_model.h5')\n\nprint(\"Model training complete and saved as alzheimers_cnn_model.h5\")\n",
            "testCaseSplitterStates": [
                [
                    310,
                    309,
                    310
                ]
            ],
            "testcasesIsShow": [
                true
            ],
            "timestamp": 1756755660237,
            "untitledIndex": 1,
            "verticalScrollbarValue": 0
        }
    ]
}
